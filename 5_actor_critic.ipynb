{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bea4aa-07ec-4801-97e9-5f884feee7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act with actor critic on moon lander "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec63c20d-cf37-4ea5-b578-edf2675e292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from utils import ReturnTracker, Agent\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a503536-060f-4915-8e65-9f819cca9207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions, nodes=256, noise=1e-6):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.nodes = nodes\n",
    "        self.fc1 = nn.Linear(n_observations, nodes)\n",
    "        self.fc2 = nn.Linear(nodes, nodes)\n",
    "        self.mu = nn.Linear(nodes, n_actions)\n",
    "        self.log_std = nn.Linear(nodes, n_actions)\n",
    "        self.noise = noise\n",
    "\n",
    "    def forward(self, observations):\n",
    "        x = F.relu(self.fc1(observations))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.mu(x)\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = torch.clamp(log_std, min=-10, max=2)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def sample(self, observations, greedy=False):\n",
    "        mu, log_std = self.forward(observations)\n",
    "        sigma = log_std.exp()\n",
    "\n",
    "        probs = Normal(mu, sigma)\n",
    "        sample = probs.rsample() if not greedy else mu\n",
    "        actions = torch.tanh(sample)\n",
    "        \n",
    "        log_probs = probs.log_prob(sample)\n",
    "        log_probs -= torch.log(1 - actions.pow(2) - self.noise)\n",
    "        log_probs = log_probs.sum(dim=1, keepdim=True)\n",
    "\n",
    "        return actions, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ed69dc-3790-46be-bef0-2245bef97800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, nodes=256):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.nodes = nodes\n",
    "        self.fc1 = nn.Linear(n_observations, nodes)\n",
    "        self.fc2 = nn.Linear(nodes, nodes)\n",
    "        self.fc3 = nn.Linear(nodes, 1)\n",
    "\n",
    "    def forward(self, observations):\n",
    "        x = F.relu(self.fc1(observations))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c1d388c-0edb-4c4a-b567-2ba0057f0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(Agent):\n",
    "    \n",
    "    def __init__(self, env, lr_policy=2e-5, lr_critic=1e-3, gamma=.99):\n",
    "        self.env = env\n",
    "        self.lr_policy = lr_policy\n",
    "        self.lr_critic = lr_critic\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.policy = PolicyNet(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "        self.policy_optim = torch.optim.Adam(self.policy.parameters(), lr=lr_policy)\n",
    "        \n",
    "        self.critic = ValueNet(env.observation_space.shape[0])\n",
    "        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        \n",
    "        self.total_num_steps = 0\n",
    "    \n",
    "    def update(self, state, action, log_prob, reward, new_state, steps):\n",
    "        \n",
    "        # train critic\n",
    "        value = self.critic(state)\n",
    "        new_value = self.critic(new_state)\n",
    "        critic_loss = F.mse_loss(reward + self.gamma*new_value.detach(), value)\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward(retain_graph=True)\n",
    "        torch.nn.utils.clip_grad_value_(self.critic.parameters(), 0.5)\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # train policy\n",
    "        td_error = reward + self.gamma * new_value.detach() - value.detach()\n",
    "        policy_loss = - td_error * log_prob  # * self.gamma ** steps \n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward(retain_graph=True)\n",
    "        torch.nn.utils.clip_grad_value_(self.policy.parameters(), 0.5)\n",
    "        self.policy_optim.step()\n",
    "        \n",
    "    \n",
    "    def learn(self, episodes, max_steps=None, tracker=None, verb=True):\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        iterator = range(episodes)\n",
    "        if verb:\n",
    "            iterator = tqdm(iterator, leave=True)\n",
    "        \n",
    "        for episode in iterator:\n",
    "\n",
    "            if tracker is not None:\n",
    "                tracker.new_episode()\n",
    "            \n",
    "            state, info = self.env.reset()\n",
    "            state = torch.tensor(state).float().reshape(1,-1)\n",
    "            \n",
    "            returns = 0\n",
    "            steps = 0\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            \n",
    "            while not terminated and not truncated:\n",
    "                \n",
    "                # pdb.set_trace()\n",
    "                action, log_prob = self.policy.sample(state)\n",
    "                \n",
    "                new_state, reward, terminated, truncated, info = self.env.step(action.flatten().detach().numpy())\n",
    "                new_state = torch.tensor(new_state).float().reshape(1,-1)\n",
    "\n",
    "                #update\n",
    "                self.update(state, action, log_prob, reward, new_state, steps)\n",
    "                \n",
    "                state = new_state\n",
    "                self.total_num_steps += 1\n",
    "                steps += 1\n",
    "                returns += reward\n",
    "                \n",
    "                if tracker is not None:\n",
    "                    tracker.add(reward)\n",
    "                \n",
    "                if max_steps is not None:\n",
    "                    if steps > max_steps:\n",
    "                        break\n",
    "            \n",
    "            if verb:\n",
    "                iterator.set_description(f\"total steps: {self.total_num_steps}, episode: {episode}, return: {returns:.4f}\")\n",
    "    \n",
    "    def predict(self, state):\n",
    "        self.policy.eval()\n",
    "        \n",
    "        state = torch.tensor(state).float()\n",
    "            \n",
    "        if len(state.shape) != 2:\n",
    "            state = state.unsqueeze(0)\n",
    "\n",
    "        action, log_probs = self.policy.sample(state, greedy=True)\n",
    "        \n",
    "        action = action.detach().numpy()\n",
    "        log_probs = log_probs.detach().numpy()\n",
    "            \n",
    "        return action, log_probs\n",
    "    \n",
    "    def train(self):\n",
    "        self.policy.train()\n",
    "        self.critic.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.policy.eval()\n",
    "        self.critic.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50dc58e7-74d7-4a14-a55d-9f65a114d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode='rgb_array').env  # \"LunarLander-v2\", continuous=True; \"Pendulum-v1\", "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0fb4aef-4b8c-40d4-b292-38e5ec94265c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d90998e-3846-48a8-be4e-afbf4863839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c33a2dde-b2e6-4093-99e5-954325b4e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ActorCritic(env, lr_policy=2e-2, lr_critic=2e-2, gamma=.99)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f150d8af-d97f-4c5f-bbf8-50f592c4d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = ReturnTracker()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b37d5e-68a8-4314-88cc-bde030b79399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207cbb90f05249beb7369239211c94a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.learn(episodes=2000,\n",
    "            max_steps=500, \n",
    "            tracker=tracker)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97b5d6-c29f-422b-b72e-a74036d14a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.plot(smooth=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29429a-fbf7-433e-b2fd-f85b3de53605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "while True:\n",
    "    \n",
    "    terminated, truncated = False, False\n",
    "    \n",
    "    state, info = env.reset()\n",
    "    \n",
    "    steps = 0\n",
    "    returns = 0\n",
    "    \n",
    "    while not terminated and not truncated:\n",
    "\n",
    "        time.sleep(.05)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        action, log_prob = agent.predict(state)\n",
    "        new_state, reward, terminated, truncated, info = env.step(action.flatten())\n",
    "\n",
    "        steps += 1\n",
    "        state = torch.tensor(new_state)\n",
    "        returns += reward\n",
    "        \n",
    "        print('steps: {}, returns: {}'.format(steps, returns))\n",
    "\n",
    "        plt.imshow(env.render())\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        if steps > 100:\n",
    "            break\n",
    "    \n",
    "    time.sleep(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af328d06-b27c-4342-93ef-c394167338c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd76fb80-b15f-4de1-9d78-817114e3b635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
